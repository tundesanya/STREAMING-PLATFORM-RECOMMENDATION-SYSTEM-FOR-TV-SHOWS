{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PczqZL0cY4GI",
        "outputId": "7be1fe08-8f23-48e4-d791-450d4c5d21f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=583849bfe89e618c96e4615900b9b513de3c13efe10c482be49a2caf254a362c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-RrUVFtXNdL"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import split, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"TVShowRecommendation\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "data = spark.read.csv(\"/content/tv_shows.csv\", header=True, inferSchema=True)\n",
        "data = data.limit(50)\n",
        "data.show()\n",
        "\n",
        "# Select columns for distribution\n",
        "columns_to_plot = [\"Netflix\", \"Hulu\", \"Prime Video\", \"Disney+\"]\n",
        "# Collect the data to the driver\n",
        "data_collect = data.select(*columns_to_plot).toPandas()\n",
        "# Plot the distributions using matplotlib\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for col in columns_to_plot:\n",
        "    plt.hist(data_collect[col], bins=20, alpha=0.5, label=col)\n",
        "\n",
        "plt.title(\"Distribution of Columns\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Split the title column into individual words\n",
        "words = udf(lambda x: x.split(), ArrayType(StringType()))\n",
        "tv_shows = data.withColumn(\"words\", words(data.Title))\n",
        "\n",
        "# Explode the array of words to create a row for each word\n",
        "tv_shows_exploded = tv_shows.select(\"ID\", \"words\", \"Title\")\n",
        "tv_shows_exploded = tv_shows_exploded.withColumn(\"word\", explode(\"words\"))\n",
        "\n",
        "\n",
        "# Index the words using StringIndexer\n",
        "string_indexer = StringIndexer(inputCol=\"word\", outputCol=\"indexed_words\")\n",
        "model = string_indexer.fit(tv_shows_exploded)\n",
        "indexed_words = model.transform(tv_shows_exploded)\n",
        "\n",
        "print(indexed_words)\n",
        "\n",
        "# Apply OneHotEncoder to the indexed words\n",
        "encoder = OneHotEncoder(inputCol=\"indexed_words\", outputCol=\"encoded_words\")\n",
        "encoder.setDropLast(False)\n",
        "ohe = encoder.fit(indexed_words) # indexer is the existing dataframe, see the question\n",
        "encoded_words = ohe.transform(indexed_words)\n",
        "\n",
        "# Assemble the indexed words into a feature vector\n",
        "assembler = VectorAssembler(inputCols=[\"encoded_words\"], outputCol=\"features\")\n",
        "#indexed_words = assembler.transform(indexed_words)\n",
        "encoded_words_assembled = assembler.transform(encoded_words)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "#(training, testing) = indexed_words.randomSplit([0.8, 0.2])\n",
        "(training, testing) = encoded_words_assembled.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the random forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"ID\", featuresCol=\"features\")\n",
        "\n",
        "# Define the evaluation metric\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"ID\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Define the hyperparameter grid for cross-validation\n",
        "param_grid = ParamGridBuilder().addGrid(rf.numTrees, [10, 50, 100]).addGrid(rf.maxDepth, [2, 5, 10]).build()\n",
        "\n",
        "\n",
        "# Define the cross-validation pipeline\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "\n",
        "# Create a CrossValidator\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "# Run cross-validation and choose the best set of parameters\n",
        "cv_model = crossval.fit(training)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = cv_model.transform(testing)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Define a function to recommend streaming platforms for a given TV show\n",
        "def recommend_platforms(title):\n",
        "    # Split the title into individual words\n",
        "    words = title.split()\n",
        "\n",
        "    # Index the words\n",
        "    indexed_wrds = model.transform(spark.createDataFrame([(words,)], [\"words\"])).select(\"indexed_words\").first()[0]\n",
        "\n",
        "    # Assemble the feature vector\n",
        "    features = assembler.transform(spark.createDataFrame([(indexed_wrds,)], [\"indexed_wrds\"])).select(\"features\").first()[0]\n",
        "\n",
        "    # Make a prediction using the trained model\n",
        "    prediction = model.transform(spark.createDataFrame([(features,)], [\"features\"])).select(\"prediction\").first()[0]\n",
        "\n",
        "    # Get the top 3 streaming platforms with the highest probabilities\n",
        "    platforms = indexed_wrds.zip(prediction).filter(lambda x: x[1] > 0).sortBy(lambda x: x[1], ascending=False).take(3)\n",
        "\n",
        "    # Return the names of the top 3 streaming platforms\n",
        "    return [model.stages[-1].labels[int(index)] for (index, prob) in platforms]\n",
        "# Test the recommendation system\n",
        "print(recommend_platforms(\"The Lion King\"))\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
